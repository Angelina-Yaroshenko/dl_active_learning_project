{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex ./packages/apex && cd ./packages/apex && pip install -v --no-cache-dir \\\n",
    "    --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../bert_sequence_tagger/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-DGXS-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import initialize_logger\n",
    "logger = initialize_logger('../workdir/logs/i2b2_active_learning.log', name='sequence_tagger_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '../workdir/cache'\n",
    "\n",
    "# MAX_LEN = 150\n",
    "MAX_LEN = 100\n",
    "\n",
    "#BATCH_SIZE = 105\n",
    "#BATCH_SIZE = 45 \n",
    "BATCH_SIZE = 32\n",
    "#BATCH_SIZE = 16\n",
    "\n",
    "MAX_N_EPOCHS = 10\n",
    "\n",
    "PRED_BATCH_SIZE = 1200\n",
    "\n",
    "#LEARNING_RATE = 3e-5\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "random_state = 2019\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "BIO_BERT = '../workdir/bio_bert/torch2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from bert_sequence_tagger import SequenceTaggerBert, ModelTrainerBert\n",
    "from flair.datasets import ColumnCorpus\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from bert_sequence_tagger.bert_utils import make_bert_tag_dict_from_flair_corpus\n",
    "from bert_sequence_tagger import BertForTokenClassificationCustom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:30:19,229 Reading data from ../workdir/i2b2/conll\n",
      "2019-10-18 03:30:19,230 Train: ../workdir/i2b2/conll/i2b2_training_hypertension.conll\n",
      "2019-10-18 03:30:19,231 Dev: None\n",
      "2019-10-18 03:30:19,232 Test: ../workdir/i2b2/conll/i2b2_testing_hypertension.conll\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 8884,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 156702,\n",
      "            \"min\": 1,\n",
      "            \"max\": 488,\n",
      "            \"avg\": 17.63867627194957\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 6813,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 117214,\n",
      "            \"min\": 1,\n",
      "            \"max\": 390,\n",
      "            \"avg\": 17.204462057830618\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 987,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 16636,\n",
      "            \"min\": 1,\n",
      "            \"max\": 332,\n",
      "            \"avg\": 16.855116514690984\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data_folder = '../workdir/i2b2/conll/'\n",
    "attr = 'hypertension'\n",
    "#attr = 'diabetes'\n",
    "#attr = 'cad'\n",
    "corpus = ColumnCorpus(data_folder, {0 : 'text', 1 : 'ner'},\n",
    "                      train_file=f'i2b2_training_{attr}.conll',\n",
    "                      test_file=f'i2b2_testing_{attr}.conll',\n",
    "                      dev_file=None)\n",
    "\n",
    "\n",
    "# data_folder = '../workdir/genia/conll/'\n",
    "# attr = 'genia'\n",
    "# corpus = ColumnCorpus(data_folder, \n",
    "#                       {0 : 'text', 1 : 'ner'},\n",
    "#                       train_file='Genia4ERtask1.iob2',\n",
    "#                       test_file='Genia4EReval1.iob2',\n",
    "#                       dev_file=None)\n",
    "\n",
    "# data_folder = '../data/conll2003/conll2003'\n",
    "# attr = 'conll2003'\n",
    "# corpus = ColumnCorpus(data_folder, \n",
    "#                       {0 : 'text', 3 : 'ner'},\n",
    "#                       train_file='eng.train.txt',\n",
    "#                       test_file='eng.testb.txt',\n",
    "#                       dev_file='eng.testa.txt')\n",
    "\n",
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N with more than max lengths: 151\n",
      "Ratio: 0.0169968482665466\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array([len(sent) for sent in corpus.train])\n",
    "n_max_lengths = (lengths > (MAX_LEN-2)).sum()\n",
    "print('N with more than max lengths:', n_max_lengths)\n",
    "print('Ratio:', n_max_lengths / lengths.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, do_lower_case=False)\n",
    "\n",
    "idx2tag, tag2idx = make_bert_tag_dict_from_flair_corpus(corpus)\n",
    "\n",
    "#model = BertForTokenClassificationCustom.from_pretrained('bert-base-cased', cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "model = BertForTokenClassificationCustom.from_pretrained(BIO_BERT, cache_dir=CACHE_DIR, num_labels=len(tag2idx)).cuda()\n",
    "\n",
    "seq_tagger = SequenceTaggerBert(bert_model=model, bpe_tokenizer=bpe_tokenizer, \n",
    "                                idx2tag=idx2tag, tag2idx=tag2idx, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59,) (59,)\n"
     ]
    }
   ],
   "source": [
    "from bert_active_learning_exp import prepare_corpus, initialize_seeds2\n",
    "\n",
    "X_train, y_train = prepare_corpus(corpus.train)\n",
    "y_seed = initialize_seeds2(y_train, ['I', 'B', 'O'], 30)\n",
    "\n",
    "selector = [e is not None for e in y_seed]\n",
    "y_train = np.array(y_seed)[selector]\n",
    "X_train = np.array(X_train)[selector]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "collate_fn = lambda inpt: tuple(zip(*inpt))\n",
    "\n",
    "train_data = list(zip(X_train, y_train))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:34:27,319 - sequence_tagger_bert - INFO - Train loss: 0.06409758169990429\n",
      "2019-10-18 03:34:27,319 Train loss: 0.06409758169990429\n",
      "2019-10-18 03:34:30,203 - sequence_tagger_bert - INFO - Validation loss: 0.02446748875081539\n",
      "2019-10-18 03:34:30,203 Validation loss: 0.02446748875081539\n",
      "2019-10-18 03:34:30,205 - sequence_tagger_bert - INFO - Validation metrics: (0.6486486486486486,)\n",
      "2019-10-18 03:34:30,205 Validation metrics: (0.6486486486486486,)\n",
      "2019-10-18 03:34:30,219 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:34:30,219 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [01:21<12:12, 81.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:35:47,347 - sequence_tagger_bert - INFO - Train loss: 0.017690664797267757\n",
      "2019-10-18 03:35:47,347 Train loss: 0.017690664797267757\n",
      "2019-10-18 03:35:50,251 - sequence_tagger_bert - INFO - Validation loss: 0.02103930525481701\n",
      "2019-10-18 03:35:50,251 Validation loss: 0.02103930525481701\n",
      "2019-10-18 03:35:50,253 - sequence_tagger_bert - INFO - Validation metrics: (0.6810551558752997,)\n",
      "2019-10-18 03:35:50,253 Validation metrics: (0.6810551558752997,)\n",
      "2019-10-18 03:35:50,268 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:35:50,268 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [02:41<10:47, 80.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:37:08,646 - sequence_tagger_bert - INFO - Train loss: 0.012191056116240596\n",
      "2019-10-18 03:37:08,646 Train loss: 0.012191056116240596\n",
      "2019-10-18 03:37:11,539 - sequence_tagger_bert - INFO - Validation loss: 0.026673121377825737\n",
      "2019-10-18 03:37:11,539 Validation loss: 0.026673121377825737\n",
      "2019-10-18 03:37:11,541 - sequence_tagger_bert - INFO - Validation metrics: (0.6858513189448441,)\n",
      "2019-10-18 03:37:11,541 Validation metrics: (0.6858513189448441,)\n",
      "2019-10-18 03:37:11,555 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:37:11,555 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [04:02<09:27, 81.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:38:27,117 - sequence_tagger_bert - INFO - Train loss: 0.0095674098272395\n",
      "2019-10-18 03:38:27,117 Train loss: 0.0095674098272395\n",
      "2019-10-18 03:38:29,998 - sequence_tagger_bert - INFO - Validation loss: 0.027471965178847313\n",
      "2019-10-18 03:38:29,998 Validation loss: 0.027471965178847313\n",
      "2019-10-18 03:38:29,999 - sequence_tagger_bert - INFO - Validation metrics: (0.6509433962264151,)\n",
      "2019-10-18 03:38:29,999 Validation metrics: (0.6509433962264151,)\n",
      "2019-10-18 03:38:30,001 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:38:30,001 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [05:21<08:01, 80.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:39:42,143 - sequence_tagger_bert - INFO - Train loss: 0.009451734010293903\n",
      "2019-10-18 03:39:42,143 Train loss: 0.009451734010293903\n",
      "2019-10-18 03:39:45,058 - sequence_tagger_bert - INFO - Validation loss: 0.033516354858875275\n",
      "2019-10-18 03:39:45,058 Validation loss: 0.033516354858875275\n",
      "2019-10-18 03:39:45,060 - sequence_tagger_bert - INFO - Validation metrics: (0.691358024691358,)\n",
      "2019-10-18 03:39:45,060 Validation metrics: (0.691358024691358,)\n",
      "2019-10-18 03:39:45,077 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:39:45,077 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [06:36<06:33, 78.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:41:02,755 - sequence_tagger_bert - INFO - Train loss: 0.007586548956705817\n",
      "2019-10-18 03:41:02,755 Train loss: 0.007586548956705817\n",
      "2019-10-18 03:41:05,690 - sequence_tagger_bert - INFO - Validation loss: 0.03357239067554474\n",
      "2019-10-18 03:41:05,690 Validation loss: 0.03357239067554474\n",
      "2019-10-18 03:41:05,692 - sequence_tagger_bert - INFO - Validation metrics: (0.7040816326530612,)\n",
      "2019-10-18 03:41:05,692 Validation metrics: (0.7040816326530612,)\n",
      "2019-10-18 03:41:05,706 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:41:05,706 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [07:56<05:17, 79.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:42:23,937 - sequence_tagger_bert - INFO - Train loss: 0.005374300019637486\n",
      "2019-10-18 03:42:23,937 Train loss: 0.005374300019637486\n",
      "2019-10-18 03:42:26,850 - sequence_tagger_bert - INFO - Validation loss: 0.033413343131542206\n",
      "2019-10-18 03:42:26,850 Validation loss: 0.033413343131542206\n",
      "2019-10-18 03:42:26,851 - sequence_tagger_bert - INFO - Validation metrics: (0.7,)\n",
      "2019-10-18 03:42:26,851 Validation metrics: (0.7,)\n",
      "2019-10-18 03:42:26,853 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:42:26,853 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [09:17<03:59, 79.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:43:44,921 - sequence_tagger_bert - INFO - Train loss: 0.004776136249700802\n",
      "2019-10-18 03:43:44,921 Train loss: 0.004776136249700802\n",
      "2019-10-18 03:43:47,840 - sequence_tagger_bert - INFO - Validation loss: 0.037290848791599274\n",
      "2019-10-18 03:43:47,840 Validation loss: 0.037290848791599274\n",
      "2019-10-18 03:43:47,842 - sequence_tagger_bert - INFO - Validation metrics: (0.6989795918367347,)\n",
      "2019-10-18 03:43:47,842 Validation metrics: (0.6989795918367347,)\n",
      "2019-10-18 03:43:47,844 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:43:47,844 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [10:38<02:40, 80.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:45:05,833 - sequence_tagger_bert - INFO - Train loss: 0.006042339135897139\n",
      "2019-10-18 03:45:05,833 Train loss: 0.006042339135897139\n",
      "2019-10-18 03:45:08,635 - sequence_tagger_bert - INFO - Validation loss: 0.034984175115823746\n",
      "2019-10-18 03:45:08,635 Validation loss: 0.034984175115823746\n",
      "2019-10-18 03:45:08,636 - sequence_tagger_bert - INFO - Validation metrics: (0.6401869158878505,)\n",
      "2019-10-18 03:45:08,636 Validation metrics: (0.6401869158878505,)\n",
      "2019-10-18 03:45:08,638 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:45:08,638 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [11:59<01:20, 80.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-18 03:46:25,671 - sequence_tagger_bert - INFO - Train loss: 0.0036513411675429303\n",
      "2019-10-18 03:46:25,671 Train loss: 0.0036513411675429303\n",
      "2019-10-18 03:46:28,520 - sequence_tagger_bert - INFO - Validation loss: 0.03348606079816818\n",
      "2019-10-18 03:46:28,520 Validation loss: 0.03348606079816818\n",
      "2019-10-18 03:46:28,522 - sequence_tagger_bert - INFO - Validation metrics: (0.6763990267639902,)\n",
      "2019-10-18 03:46:28,522 Validation metrics: (0.6763990267639902,)\n",
      "2019-10-18 03:46:28,524 - sequence_tagger_bert - INFO - Current learning rate: 5e-05\n",
      "2019-10-18 03:46:28,524 Current learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [13:19<00:00, 79.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from bert_sequence_tagger.bert_utils import create_loader_from_flair_corpus, get_parameters_without_decay\n",
    "from bert_sequence_tagger.model_trainer_bert import ModelTrainerBert\n",
    "\n",
    "from bert_sequence_tagger.metrics import f1_entity_level, f1_token_level\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "train_dataloader = create_loader_from_flair_corpus(corpus.train, \n",
    "                                                   sampler_ctor=RandomSampler, \n",
    "                                                   batch_size=BATCH_SIZE)\n",
    "valid_dataloader = create_loader_from_flair_corpus(corpus.dev,\n",
    "                                                   sampler_ctor=SequentialSampler,\n",
    "                                                   batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(get_parameters_without_decay(model), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "                  eps =1e-6, weight_decay=0.01, correct_bias=True)\n",
    "\n",
    "# lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0.1, \n",
    "#                                     t_total=(len(corpus.train) / BATCH_SIZE)*MAX_N_EPOCHS)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "trainer = ModelTrainerBert(model=seq_tagger, \n",
    "                           optimizer=optimizer, \n",
    "                           lr_scheduler=lr_scheduler,\n",
    "                           train_dataloader=train_dataloader, \n",
    "                           val_dataloader=valid_dataloader,\n",
    "                           update_scheduler='ep',\n",
    "                           keep_best_model=True,\n",
    "                           restore_bm_on_lr_change=True,\n",
    "                           max_grad_norm=1.,\n",
    "                           validation_metrics=[f1_entity_level],\n",
    "                           decision_metric=lambda metrics: -metrics[1])\n",
    "\n",
    "trainer.train(epochs=MAX_N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.021918293243894976, 0.7372134038800705, 0.8101604278074866)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = create_loader_from_flair_corpus(corpus.test, \n",
    "                                                  sampler_ctor=SequentialSampler,\n",
    "                                                  batch_size=PRED_BATCH_SIZE)\n",
    "\n",
    "pred1, __, metrics = seq_tagger.predict(test_dataloader, evaluate=True, metrics=[f1_entity_level, f1_token_level])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5357271095152604"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "from bert_active_learning_exp import prepare_corpus\n",
    "\n",
    "X_test, y_test = prepare_corpus(corpus.test)\n",
    "\n",
    "test_sampler = SequentialSampler(X_test)\n",
    "test_dataloader = DataLoader(X_test, \n",
    "                             sampler=test_sampler, \n",
    "                             batch_size=1200,\n",
    "                             collate_fn = lambda inpt: inpt)\n",
    "\n",
    "pred, proba = seq_tagger.predict(test_dataloader, evaluate=False)\n",
    "\n",
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genia: 0.736 0.7399374123232978  \n",
    "CoNLL: 0.917   \n",
    "Diabetes: 0.744  0.7389112903225806\n",
    "Hypertension: 0.7357512953367875 0.7510584250635056 0.7366623986342296 0.7434072833821682 \n",
    "CAD: 0.381 0.4272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../workdir/models/bert/{attr}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from i2b2_utils import drop_noise_samples\n",
    "\n",
    "dataset_test_path = '../workdir/i2b2/i2b2_testing.json'\n",
    "dataset_test = pd.read_json(dataset_test_path)\n",
    "dataset_test.head()\n",
    "test_selected_dataset = drop_noise_samples(dataset_test, attr.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "def flair_process_i2b2(model, dataset, attr_name):\n",
    "    res = model.predict([Sentence(t) for t in dataset.texts])\n",
    "    res = [[e.tags[attr_name].value for e in sent] for sent in res]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-06 15:53:14,376 loading file ../models/new/CAD//elmo-pubmed/1.0/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "#model = SequenceTagger.load('../models/new/DIABETES/fasttext/1.0/best-model.pt')\n",
    "#model = SequenceTagger.load('../models/new/HYPERTENSION//elmo-pubmed/1.0/best-model.pt')\n",
    "model = SequenceTagger.load('../models/new/CAD//elmo-pubmed/1.0/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_results = flair_process_i2b2(model, test_selected_dataset, attr_name)\n",
    "pos, pred_pos, tp = evaluation_level_document(flair_results, test_selected_dataset, attr_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from i2b2_utils import evaluation_level_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bert\n",
    "from bert_utils import annotate_text \n",
    "\n",
    "pred_tags = annotate_text(loaded_model, test_dataloader, tags_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, pred_pos, tp = evaluation_level_document(pred_tags, test_selected_dataset, attr.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.9821428571428571\n",
      "Precision:  0.9553349875930521\n",
      "F1: 0.9685534591194969\n"
     ]
    }
   ],
   "source": [
    "recall = tp / pos\n",
    "precision = tp / pred_pos\n",
    "f1 = 2. * recall * precision / (recall + precision)\n",
    "\n",
    "print('Recall: ', recall)\n",
    "print('Precision: ', precision)\n",
    "print('F1:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hypertension:\n",
    "Recall:  0.9719387755102041\n",
    "Precision:  0.9645569620253165\n",
    "F1: 0.9682337992376113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAD:\n",
    "Recall:  0.9511111111111111\n",
    "Precision:  0.6793650793650794\n",
    "F1: 0.7925925925925925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diabetes:\n",
    "Recall:  0.9667590027700831\n",
    "Precision:  0.8790931989924433\n",
    "F1: 0.9208443271767809"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
